{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "news_classification_EDA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZzOoOU2Md15"
      },
      "source": [
        "Построить модель для выявления в новости события, соответствующего задержке ввода некоторого объекта в эксплуатацию."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJtsIBvCMj0G"
      },
      "source": [
        "###EDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0zXfVl11NP9"
      },
      "source": [
        "%%capture\n",
        "!wget https://raw.githubusercontent.com/shitkov/news_classification/main/news.zip\n",
        "!unzip '/content/news.zip' -d '/content/'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MliM_4tH4zW1"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcCn6HwYLW4t"
      },
      "source": [
        "data = pd.read_csv('/content/train_data.csv', index_col=0)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqqwL5tELjlE",
        "outputId": "e6f0ac2f-3299-41d5-f220-dd044718d757"
      },
      "source": [
        "# баланс классов\n",
        "print('1: ', len(data[data['label'] == 1]))\n",
        "print('0: ', len(data[data['label'] == 0]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1:  329\n",
            "0:  1340\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BjKkGQsOHT-"
      },
      "source": [
        "# чистка данных\n",
        "import re\n",
        "texts = list(data['sentence'])\n",
        "labels = list(data['label'])\n",
        "# убрать лишние символы\n",
        "texts = [re.sub('[^а-яё ]', ' ', str(t).lower()) for t in texts]\n",
        "# убрать лишние пробелы\n",
        "texts = [re.sub(r\" +\", \" \", t).strip() for t in texts]\n",
        "data['sentence'] = texts"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9HeabbmMH87"
      },
      "source": [
        "# train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "data_train, data_test = train_test_split(data, test_size=0.3, random_state=42)\n",
        "labels_train = list(data_train['label'])\n",
        "labels_test = list(data_test['label'])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRtZSxUIpFLD"
      },
      "source": [
        "### Vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iP-LwvuOv-Z1"
      },
      "source": [
        "TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mPzB7XeHUVD"
      },
      "source": [
        "# установка лемматизатора\n",
        "%%capture\n",
        "!pip install pymystem3\n",
        "from pymystem3 import Mystem\n",
        "mstm = Mystem()\n",
        "!wget http://download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz\n",
        "!tar -xvf mystem-3.0-linux3.1-64bit.tar.gz\n",
        "!cp mystem /root/.local/bin/mystem"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhSpS6dOJqTp"
      },
      "source": [
        "# лемматизация\n",
        "mstm = Mystem()\n",
        "\n",
        "texts_train = list(data_train['sentence'])\n",
        "texts_test = list(data_test['sentence'])\n",
        "\n",
        "texts_train_norm = [''.join(mstm.lemmatize(t)[:-1]) for t in texts_train]\n",
        "texts_test_norm = [''.join(mstm.lemmatize(t)[:-1]) for t in texts_test]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHokFHOzJh1_"
      },
      "source": [
        "# стоп-слова\n",
        "%%capture\n",
        "!wget https://raw.githubusercontent.com/shitkov/news_classification/main/rus_stop_dict.txt"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_ucWmwkT4qB"
      },
      "source": [
        "path_stop = '/content/rus_stop_dict.txt'\n",
        "\n",
        "try:\n",
        "    with open(path_stop) as f:\n",
        "        stopwords = [line.rstrip('\\n') for line in f]\n",
        "except Exception as err:\n",
        "    print(err)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkQi0_KPULi4"
      },
      "source": [
        "# удаление стоп-слов\n",
        "def drop_stop(text):\n",
        "    tokens = text.split(' ')\n",
        "    tokens = [t for t in tokens if t not in stopwords]\n",
        "    return ' '.join(tokens)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIExz9WOKXEl"
      },
      "source": [
        "texts_train_norm_clean = [drop_stop(t) for t in texts_train_norm]\n",
        "texts_test_norm_clean = [drop_stop(t) for t in texts_test_norm]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9et6GmayFqWW"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "model_tfidf = TfidfVectorizer(max_features=10000)\n",
        "X_train_tfidf = model_tfidf.fit_transform(texts_train_norm_clean)\n",
        "X_test_tfidf = model_tfidf.transform(texts_test_norm_clean)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wnoa9y9jwCQy"
      },
      "source": [
        "FastText"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwwHM9zmbsBm"
      },
      "source": [
        "%%capture\n",
        "!pip install gensim==3.8.1\n",
        "!pip install compress-fasttext[full]\n",
        "\n",
        "import gensim\n",
        "import compress_fasttext\n",
        "import numpy as np\n",
        "model_fasttext = compress_fasttext.models.CompressedFastTextKeyedVectors.load(\n",
        "    'https://github.com/avidale/compress-fasttext/releases/download/v0.0.1/ft_freqprune_400K_100K_pq_300.bin'\n",
        ")\n",
        "\n",
        "X_train_fasttext = np.asarray([model_fasttext[text] for text in texts_train])\n",
        "X_test_fasttext = np.asarray([model_fasttext[text] for text in texts_test])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcEOoi_OwGCh"
      },
      "source": [
        "USE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQjZpSlmbsGn"
      },
      "source": [
        "!pip3 install tensorflow_text>=2.0.0rc0\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "import tensorflow_text\n",
        "\n",
        "model_use = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\")\n",
        "\n",
        "X_train_use = np.asarray([model_use(text)[0] for text in texts_train])\n",
        "X_test_use = np.asarray([model_use(text)[0] for text in texts_test])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WJ9r8r9B6a3"
      },
      "source": [
        "BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-tFweg1B5pW"
      },
      "source": [
        "%%capture\n",
        "!pip install transformers"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ojZGUp3B_xo"
      },
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FvaFtaCCEvk"
      },
      "source": [
        "def bert_emb(sentences, tokenizer, model, device):\n",
        "    emb_list = []\n",
        "    for s in sentences:\n",
        "        encoding = tokenizer(s, add_special_tokens=True, return_tensors='pt', padding='max_length', max_length=128, truncation=True)\n",
        "        input_ids = encoding.input_ids.to(device)\n",
        "        token_type_ids = encoding.token_type_ids.to(device)\n",
        "        attention_mask = encoding.attention_mask.to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                token_type_ids=token_type_ids\n",
        "                )\n",
        "        \n",
        "        embeddings = outputs.last_hidden_state[:,0,:].cpu().detach().numpy()\n",
        "        emb_list.append(embeddings[0])\n",
        "    return np.array(emb_list)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQiZRYFmCEyA",
        "outputId": "8ba2a5d4-cfeb-4283-99dc-c3dbe87a5713"
      },
      "source": [
        "%%capture\n",
        "tokenizer_bert = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "model_bert = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_bert.to(device)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDq9zdoICYDy"
      },
      "source": [
        "X_train_bert = bert_emb(texts_train, tokenizer_bert, model_bert, device)\n",
        "X_test_bert = bert_emb(texts_test, tokenizer_bert, model_bert, device)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2-DPgtjEmL8"
      },
      "source": [
        "data_dict = {\n",
        "    'tfidf': (X_train_tfidf, X_test_tfidf),\n",
        "    'fasttext': (X_train_fasttext, X_test_fasttext),\n",
        "    'use': (X_train_use, X_test_use),\n",
        "    'bert': (X_train_bert, X_test_bert)}"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_F9GjAdpBSK"
      },
      "source": [
        "### Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rz6FLq2AsZZM"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "class Classifier:\n",
        "\n",
        "    def __init__(self, classifier='logreg'):\n",
        "        if classifier == 'random_forest':\n",
        "             self.cls = RandomForestClassifier(random_state=42)\n",
        "        elif classifier == 'xgboost':\n",
        "             self.cls = XGBClassifier(random_state=42, )\n",
        "        else:\n",
        "            self.cls = LogisticRegression(\n",
        "                random_state=42,\n",
        "                max_iter=10000,\n",
        "                class_weight={0:1.0, 1:2.0}\n",
        "                )\n",
        "\n",
        "    def predict(self, x_train, x_test, y_train, y_test):\n",
        "        self.cls.fit(x_train, y_train)\n",
        "        predictions = self.cls.predict(x_test)\n",
        "        return f1_score(predictions, y_test)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebqV45Ae8RwQ"
      },
      "source": [
        "cls_list = ['logreg', 'random_forest', 'xgboost']"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77RMA8rU8518",
        "outputId": "53bb7205-0d31-4920-af69-ad7001a4ca3d"
      },
      "source": [
        "results = []\n",
        "for emb in data_dict.keys():\n",
        "    x_train, x_test = data_dict[emb]\n",
        "    for cls_name in cls_list:\n",
        "        score = Classifier(cls_name).predict(x_train, x_test, labels_train, labels_test)\n",
        "        results.append((emb + '_' + cls_name, score))\n",
        "\n",
        "results = sorted(results, key=lambda x: x[1], reverse=True)\n",
        "print('BEST: ', results[0])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BEST:  ('tfidf_xgboost', 0.650887573964497)\n"
          ]
        }
      ]
    }
  ]
}